/**
 * LLMManager - LLM ç®¡ç†å™¨ï¼ˆæ–°æ¶æ„ï¼‰
 *
 * ä½¿ç”¨ä¸¤çº§é…ç½®ç»“æ„ï¼ˆæä¾›å•† + æ¨¡å‹ï¼‰
 * æ”¯æŒå¤šæ¨¡å‹ç±»å‹ï¼ˆNLP, Embedding, Rerank ç­‰ï¼‰
 * é…ç½®ä» SQLite æ•°æ®åº“åŠ è½½ï¼Œæ”¯æŒè¿è¡Œæ—¶çƒ­æ›´æ–°
 */

import { Message, ChatOptions, LLMResponse } from "../types";
import { logger } from "../utils/logger";
import { LLMConfigService } from "../services/LLMConfigService";
import { ModelRegistry } from "../services/ModelRegistry";
import { LLMModelType, LLMModelFull } from "../types/llm-models";
import { buildApiUrl } from "../config/endpoint-mappings";
import { LLMAdapterFactory, ILLMAdapter } from "./llm/adapters";
import { LIMITS, TIMEOUT, DOOM_LOOP } from "../constants";

/**
 * é€‚é…å™¨ç¼“å­˜æ¡ç›®
 */
interface AdapterCacheEntry {
  adapter: ILLMAdapter;
  configHash: string;
  lastUsed: number;
}

/**
 * LLM ç®¡ç†å™¨ï¼ˆæ–°æ¶æ„ï¼‰
 */
export class LLMManager {
  // æä¾›å•†çº§åˆ«é€‚é…å™¨ç¼“å­˜ï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼‰
  private adapters: Map<string, ILLMAdapter> = new Map();
  // æ¨¡å‹çº§åˆ«é€‚é…å™¨ç¼“å­˜ï¼ˆåŠ¨æ€åˆ›å»ºï¼ŒæŒ‰éœ€ç¼“å­˜ï¼‰
  private modelAdapterCache: Map<string, AdapterCacheEntry> = new Map();
  private modelRegistry: ModelRegistry;
  private configService: LLMConfigService;

  // ç¼“å­˜é…ç½®
  private readonly MAX_CACHE_SIZE = LIMITS.ADAPTER_CACHE_SIZE;
  private readonly CACHE_TTL_MS = TIMEOUT.ADAPTER_CACHE_TTL;

  constructor() {
    this.configService = LLMConfigService.getInstance();
    this.modelRegistry = ModelRegistry.getInstance();

    logger.debug("ğŸ¤– Initializing LLM Manager (new architecture)...");
    this.loadProviders();
  }

  /**
   * ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å¯ç”¨çš„æä¾›å•†
   */
  private async loadProviders(): Promise<void> {
    try {
      const providers = (await this.configService.listProviders()).filter((p) => p.enabled);

      if (providers.length === 0) {
        logger.warn("âš ï¸  No enabled LLM providers found");
        return;
      }

      // ä¸ºæ¯ä¸ªæä¾›å•†åˆ›å»ºé€‚é…å™¨
      for (const provider of providers) {
        try {
          // ä½¿ç”¨æä¾›å•†çš„ baseConfig åˆ›å»ºé€‚é…å™¨
          const adapter = LLMAdapterFactory.create(provider.provider, {
            apiKey: provider.baseConfig.apiKey,
            baseURL: provider.baseConfig.baseURL,
            defaultModel: "", // æ¨¡å‹ç”±è°ƒç”¨æ—¶æŒ‡å®š
            timeout: provider.baseConfig.timeout,
            maxRetries: provider.baseConfig.maxRetries,
          });

          this.adapters.set(provider.provider, adapter);
          logger.debug(`Loaded provider: ${provider.provider} (${provider.name})`);
        } catch (error: any) {
          logger.error(`âŒ Failed to create adapter for ${provider.provider}:`, error.message);
        }
      }

      logger.debug(`Loaded ${this.adapters.size} LLM providers`);
    } catch (error: any) {
      logger.error("âŒ Failed to load providers:", error);
    }
  }

  /**
   * èŠå¤©è¡¥å…¨ï¼ˆè‡ªåŠ¨é€‰æ‹© NLP æ¨¡å‹ï¼‰- ä½¿ç”¨é€‚é…å™¨ç¼“å­˜ä¼˜åŒ–æ€§èƒ½
   */
  async chat(messages: Message[], options?: ChatOptions): Promise<LLMResponse> {
    try {
      // 1. ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
      let model: LLMModelFull | null = null;

      if (options?.provider && options?.model) {
        // æŒ‡å®šäº†æä¾›å•†å’Œæ¨¡å‹
        model = this.modelRegistry.findModel(options.provider, options.model);
      } else if (options?.provider) {
        // åªæŒ‡å®šäº†æä¾›å•†ï¼Œä½¿ç”¨è¯¥æä¾›å•†çš„é»˜è®¤ NLP æ¨¡å‹
        const provider = await this.configService.getProviderByKey(options.provider);
        if (provider) {
          const models = await this.configService.listModels({
            providerId: provider.id,
            modelType: LLMModelType.NLP,
            isDefault: true,
            enabled: true,
          });
          model = models[0] || null;
        }
      } else {
        // ä½¿ç”¨é»˜è®¤ NLP æ¨¡å‹
        model = this.modelRegistry.getDefaultModel(LLMModelType.NLP);
      }

      if (!model) {
        throw new Error("No NLP model available");
      }

      // 2. è·å–æˆ–åˆ›å»ºé€‚é…å™¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
      const adapter = await this.getOrCreateModelAdapter(model);

      // 3. è°ƒç”¨èŠå¤©
      logger.debug(`ğŸ’¬ Using model: ${model.modelName} (${model.provider}/${model.modelKey})`);

      return await adapter.chat(messages, {
        ...options,
        model: model.modelKey,
      });
    } catch (error: any) {
      logger.error("âŒ Chat failed:", error);
      throw error;
    }
  }

  /**
   * è·å–æˆ–åˆ›å»ºæ¨¡å‹çº§åˆ«çš„é€‚é…å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
   */
  private async getOrCreateModelAdapter(model: LLMModelFull): Promise<ILLMAdapter> {
    const cacheKey = `${model.provider}:${model.modelKey}`;
    const configHash = this.computeConfigHash(model);

    let entry = this.modelAdapterCache.get(cacheKey);

    // æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if (
      entry &&
      entry.configHash === configHash &&
      Date.now() - entry.lastUsed < this.CACHE_TTL_MS
    ) {
      entry.lastUsed = Date.now();
      logger.debug(`[LLMManager] Cache hit for adapter: ${cacheKey}`);
      return entry.adapter;
    }

    // åˆ›å»ºæ–°é€‚é…å™¨
    const apiUrl = model.apiEndpointSuffix
      ? buildApiUrl(model.providerBaseConfig.baseURL, model.apiEndpointSuffix)
      : model.providerBaseConfig.baseURL;

    const adapter = LLMAdapterFactory.create(model.provider, {
      apiKey: model.providerBaseConfig.apiKey,
      baseURL: apiUrl,
      defaultModel: model.modelKey,
      timeout: model.providerBaseConfig.timeout || TIMEOUT.LLM_REQUEST,
      maxRetries: model.providerBaseConfig.maxRetries || DOOM_LOOP.THRESHOLD,
    });

    // æ›´æ–°ç¼“å­˜
    if (this.modelAdapterCache.size >= this.MAX_CACHE_SIZE) {
      this.evictOldestEntry();
    }
    this.modelAdapterCache.set(cacheKey, {
      adapter,
      configHash,
      lastUsed: Date.now(),
    });

    logger.debug(`[LLMManager] Created new adapter for: ${cacheKey}`);
    return adapter;
  }

  /**
   * è®¡ç®—æ¨¡å‹é…ç½®å“ˆå¸Œå€¼ï¼ˆç”¨äºæ£€æµ‹é…ç½®å˜åŒ–ï¼‰
   */
  private computeConfigHash(model: LLMModelFull): string {
    const configStr = JSON.stringify({
      apiKey: model.providerBaseConfig.apiKey,
      baseURL: model.providerBaseConfig.baseURL,
      apiEndpointSuffix: model.apiEndpointSuffix,
      modelKey: model.modelKey,
      timeout: model.providerBaseConfig.timeout,
      maxRetries: model.providerBaseConfig.maxRetries,
    });
    return this.simpleHash(configStr);
  }

  /**
   * ç®€å•çš„å­—ç¬¦ä¸²å“ˆå¸Œå‡½æ•°
   */
  private simpleHash(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return hash.toString(16);
  }

  /**
   * é©±é€æœ€æ—§çš„ç¼“å­˜æ¡ç›®
   */
  private evictOldestEntry(): void {
    let oldestKey: string | null = null;
    let oldestTime = Infinity;

    for (const [key, entry] of this.modelAdapterCache.entries()) {
      if (entry.lastUsed < oldestTime) {
        oldestTime = entry.lastUsed;
        oldestKey = key;
      }
    }

    if (oldestKey) {
      this.modelAdapterCache.delete(oldestKey);
      logger.debug(`[LLMManager] Evicted oldest cache entry: ${oldestKey}`);
    }
  }

  /**
   * æµå¼èŠå¤©è¡¥å…¨
   */
  async *streamChat(
    messages: Message[],
    options?: ChatOptions,
    abortSignal?: AbortSignal
  ): AsyncIterableIterator<string> {
    logger.debug(`[LLMManager.streamChat] Input options: ${JSON.stringify(options)}`);

    const model = await this.getActiveModel(options);

    if (!model) {
      throw new Error("No NLP model available");
    }

    const adapter = await this.getOrCreateAdapter(model);

    logger.debug(
      `ğŸ’¬ Streaming with model: ${model.modelName} (${model.provider}/${model.modelKey})`
    );

    // è°ƒç”¨é€‚é…å™¨çš„ streamChat æ–¹æ³•
    // âœ… ä¿®å¤ï¼šæ­£ç¡®ä¼ é€’å‚æ•°ï¼ˆæ²¡æœ‰toolsï¼‰
    yield* adapter.streamChat(
      messages,
      {
        ...options,
        model: model.modelKey,
      },
      undefined,
      abortSignal
    );
  }

  /**
   * è·å–æ´»è·ƒçš„æ¨¡å‹ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰
   */
  private async getActiveModel(options?: ChatOptions): Promise<LLMModelFull | null> {
    logger.debug(
      `[LLMManager.getActiveModel] Input options: provider=${options?.provider}, model=${options?.model}`
    );

    if (options?.provider && options?.model) {
      logger.debug(
        `[LLMManager.getActiveModel] Searching for model: ${options.provider}/${options.model}`
      );
      const foundModel = this.modelRegistry.findModel(options.provider, options.model);
      logger.debug(`[LLMManager.getActiveModel] Found model: ${foundModel?.modelName || "null"}`);
      return foundModel;
    } else if (options?.provider) {
      const provider = await this.configService.getProviderByKey(options.provider);
      if (provider) {
        const models = await this.configService.listModels({
          providerId: provider.id,
          modelType: LLMModelType.NLP,
          isDefault: true,
          enabled: true,
        });
        return models[0] || null;
      }
    }

    logger.debug("[LLMManager.getActiveModel] Using system default model");
    return this.modelRegistry.getDefaultModel(LLMModelType.NLP);
  }

  /**
   * è·å–æˆ–åˆ›å»ºé€‚é…å™¨ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰
   */
  private async getOrCreateAdapter(model: LLMModelFull): Promise<ILLMAdapter> {
    const adapter = this.adapters.get(model.provider);
    if (adapter) {
      return adapter;
    }

    // åŠ¨æ€åˆ›å»ºé€‚é…å™¨
    const apiUrl = model.apiEndpointSuffix
      ? buildApiUrl(model.providerBaseConfig.baseURL, model.apiEndpointSuffix)
      : model.providerBaseConfig.baseURL;

    const freshAdapter = LLMAdapterFactory.create(model.provider, {
      apiKey: model.providerBaseConfig.apiKey,
      baseURL: apiUrl,
      defaultModel: model.modelKey,
      timeout: model.providerBaseConfig.timeout || TIMEOUT.LLM_REQUEST,
      maxRetries: model.providerBaseConfig.maxRetries || DOOM_LOOP.THRESHOLD,
    });

    this.adapters.set(model.provider, freshAdapter);
    return freshAdapter;
  }

  /**
   * æ–‡æœ¬å‘é‡åŒ–ï¼ˆä½¿ç”¨ Embedding æ¨¡å‹ï¼‰
   * é‡‡ç”¨ä¸¤çº§ä¼˜å…ˆçº§é€‰æ‹©æ¨¡å‹ï¼š
   * 1. ä¼˜å…ˆçº§1ï¼šSQLite ä¸­é…ç½®çš„é»˜è®¤ embedding æ¨¡å‹ï¼ˆis_default = 1ï¼‰
   * 2. ä¼˜å…ˆçº§2ï¼š.env é…ç½®ä¸­çš„ EMBEDDING_PROVIDER å’Œ EMBEDDING_MODEL
   */
  async embed(texts: string[]): Promise<number[][]> {
    try {
      // 1. ä¼˜å…ˆçº§1ï¼šSQLite å…¨å±€é»˜è®¤ embedding æ¨¡å‹
      let model = this.modelRegistry.getDefaultModel(LLMModelType.EMBEDDING);

      // 2. ä¼˜å…ˆçº§2ï¼šå›é€€åˆ° .env é…ç½®
      if (!model) {
        const envProvider = process.env.EMBEDDING_PROVIDER;
        const envModel = process.env.EMBEDDING_MODEL;

        if (envProvider && envModel) {
          model = this.modelRegistry.findModel(envProvider, envModel);
          if (model) {
            logger.info(`[LLMManager] Using .env embedding config: ${envProvider}/${envModel}`);
          }
        } else if (envModel && !envProvider) {
          // å°è¯•ä»æ¨¡å‹åç§°æ¨æ–­ provider
          const match = envModel.match(/^([a-zA-Z0-9]+)-/);
          if (match) {
            const inferredProvider = match[1];
            logger.info(
              `[LLMManager] Using .env model with inferred provider: ${inferredProvider}/${envModel}`
            );
            model = this.modelRegistry.findModel(inferredProvider, envModel);
          }
        }
      }

      // 3. éªŒè¯æ¨¡å‹å¯ç”¨æ€§
      if (!model) {
        throw new Error(
          "No embedding model available. " +
            "Please configure an embedding model in SQLite (set is_default=1) or set EMBEDDING_PROVIDER and EMBEDDING_MODEL in .env"
        );
      }

      // 4. è·å–å¯¹åº”çš„é€‚é…å™¨
      const adapter = this.adapters.get(model.provider);
      if (!adapter) {
        throw new Error(`No adapter found for provider: ${model.provider}`);
      }

      // 5. æ£€æŸ¥é€‚é…å™¨æ˜¯å¦æ”¯æŒ embed æ–¹æ³•
      if (!adapter.embed) {
        throw new Error(`Adapter for ${model.provider} does not support embedding`);
      }

      // 6. è°ƒç”¨ Embedding API
      logger.debug(
        `ğŸ”¢ Using embedding model: ${model.modelName} (${model.provider}/${model.modelKey})`
      );

      const embeddings = await adapter.embed(texts, model.modelKey);

      logger.debug(
        `âœ… Generated ${embeddings.length} embeddings with ${embeddings[0]?.length || 0} dimensions`
      );

      return embeddings;
    } catch (error: any) {
      logger.error("âŒ Embed failed:", error);
      throw error;
    }
  }

  /**
   * åˆ·æ–°é…ç½®ï¼ˆé‡æ–°åŠ è½½æä¾›å•†ï¼‰
   */
  public refresh(): void {
    logger.info("ğŸ”„ Refreshing LLM Manager...");
    this.adapters.clear();
    this.modelAdapterCache.clear();
    this.loadProviders();
    this.modelRegistry.forceRefresh();
  }

  /**
   * è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨
   */
  public getAvailableProviders(): string[] {
    return Array.from(this.adapters.keys());
  }

  /**
   * æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  public hasProvider(provider: string): boolean {
    return this.adapters.has(provider);
  }

  /**
   * æ›´æ–°æä¾›å•†é…ç½®ï¼ˆæ•°æ®åº“ + å†…å­˜ï¼‰
   */
  async updateProvider(id: number, input: any): Promise<void> {
    // æ›´æ–°æ•°æ®åº“
    this.configService.updateProvider(id, input);

    // åˆ·æ–°å†…å­˜
    this.refresh();
  }

  /**
   * è·å–æ‰€æœ‰æ¨¡å‹ï¼ˆç”¨äº APIï¼‰
   */
  public getAllModels(): Array<{ id: string; provider: string; model: string; type: string }> {
    const models = this.modelRegistry.getAllModels();
    return models.map((m) => ({
      id: `${m.provider}/${m.modelKey}`,
      provider: m.provider,
      model: m.modelKey,
      type: m.modelType,
    }));
  }
}
