/**
 * BaseAdapter - LLMé€‚é…å™¨åŸºç±»
 * æä¾›é€šç”¨çš„OpenAIå…¼å®¹é€‚é…å™¨å®ç°
 */

import axios, { AxiosInstance } from "axios";
import { Message, ChatOptions, LLMResponse, LLMProviderConfig } from "../../../types";
import { logger } from "../../../utils/logger";
import { retry, RetryConfig } from "../../../utils/retry";
import { logErrorResponse, createErrorMessage } from "../../../utils/error-serializer";

/**
 * Axios è¯·æ±‚é…ç½®æ¥å£
 */
export interface AxiosRequestConfig {
  baseURL: string;
  headers: Record<string, string>;
  timeout: number;
  proxy?:
    | false
    | {
        host: string;
        port: number;
        protocol?: string;
        auth?: {
          username: string;
          password: string;
        };
      };
}

/**
 * OpenAI å…¼å®¹ API è¯·æ±‚ä½“æ¥å£
 */
export interface OpenAIRequestBody {
  model: string;
  messages: Array<{
    role: string;
    content:
      | string
      | Array<{
          type: string;
          text?: string;
          image_url?: string | { url: string };
        }>;
    name?: string;
  }>;
  stream: boolean;
  temperature?: number;
  top_p?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  repetition_penalty?: number;
  seed?: number;
  logit_bias?: Record<string, number>;
  max_tokens?: number;
  response_format?: { type: string };
  stop?: string[];
  tools?: unknown[];
  tool_choice?: string;
}

/**
 * OpenAI å…¼å®¹ API å“åº”æ¥å£
 */
export interface OpenAIResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message?: {
      role: string;
      content: string;
      tool_calls?: unknown[];
    };
    finish_reason?: string;
  }>;
  usage?: {
    prompt_tokens?: number;
    completion_tokens?: number;
    total_tokens?: number;
  };
}

/**
 * LLMé€‚é…å™¨æ¥å£
 */
export interface ILLMAdapter {
  chat(messages: Message[], options: ChatOptions, signal?: AbortSignal): Promise<LLMResponse>;
  streamChat(
    messages: Message[],
    options: ChatOptions,
    tools?: any[],
    signal?: AbortSignal
  ): AsyncIterableIterator<string>;
  getModels(): Promise<string[]>;
  embed?(texts: string[], model?: string): Promise<number[][]>;
}

/**
 * OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
 */
export abstract class BaseOpenAICompatibleAdapter implements ILLMAdapter {
  protected client: AxiosInstance;
  protected providerName: string;
  protected config: LLMProviderConfig;

  constructor(providerName: string, config: LLMProviderConfig) {
    this.providerName = providerName;
    this.config = config;

    // æ„å»ºaxiosé…ç½®
    const axiosConfig: AxiosRequestConfig = {
      baseURL: config.baseURL,
      headers: {
        ...(config.apiKey && { Authorization: `Bearer ${config.apiKey}` }),
        "Content-Type": "application/json",
      },
      timeout: config.timeout || 60000,
    };

    // å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†proxyï¼Œä½¿ç”¨å®ƒ
    if (config.proxy !== undefined) {
      axiosConfig.proxy = config.proxy;
    }

    this.client = axios.create(axiosConfig);

    logger.debug(
      `${providerName} adapter initialized (${config.baseURL}${config.proxy === false ? ", proxy disabled" : ""})`
    );
  }

  /**
   * è¿‡æ»¤é€‰é¡¹ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰
   */
  protected filterOptions(options: ChatOptions): ChatOptions {
    return options;
  }

  /**
   * æ„å»ºè¯·æ±‚ä½“ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰
   * ğŸ†• æ”¯æŒæ–°çš„é…ç½®ç»“æ„
   * ğŸ†• æ”¯æŒå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰
   */
  protected buildRequestBody(messages: Message[], options: ChatOptions): OpenAIRequestBody {
    const { provider, ...apiOptions } = options;
    const filteredOptions = this.filterOptions(apiOptions);

    // ğŸ¾ å¤„ç†æ¶ˆæ¯æ ¼å¼ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
    const processedMessages = messages.map((msg) => {
      if (Array.isArray(msg.content)) {
        // å¤šæ¨¡æ€æ¶ˆæ¯ï¼šè½¬æ¢ä¸ºOpenAIå…¼å®¹æ ¼å¼
        return {
          ...msg,
          content: msg.content.map((part) => {
            if (part.type === "image_url") {
              return {
                type: "image_url",
                image_url: part.image_url,
              };
            }
            return {
              type: "text",
              text: part.text || "",
            };
          }),
        };
      }
      // çº¯æ–‡æœ¬æ¶ˆæ¯
      return {
        ...msg,
        content: msg.content,
      };
    });

    // ğŸ¾ æ„å»ºåŸºç¡€è¯·æ±‚ä½“
    const requestBody: OpenAIRequestBody = {
      model: options.model || this.config.defaultModel,
      messages: processedMessages as OpenAIRequestBody["messages"],
      stream: false,
      temperature: options.temperature,
    };

    // ğŸ¾ å¤„ç†æ¸©åº¦å‚æ•°ï¼ˆåŸºç¡€é…ç½®ï¼‰
    if (options.temperature !== undefined) {
      requestBody.temperature = options.temperature;
    }

    // ğŸ¾ å¤„ç†ç”Ÿæˆé…ç½®ï¼ˆGenerationConfigï¼‰
    if (options.generationConfig) {
      const gc = options.generationConfig;

      // Top-P é‡‡æ ·
      if (gc.topP !== undefined) {
        requestBody.top_p = gc.topP;
      }

      // é¢‘ç‡æƒ©ç½š
      if (gc.frequencyPenalty !== undefined) {
        requestBody.frequency_penalty = gc.frequencyPenalty;
      }

      // å­˜åœ¨æƒ©ç½š
      if (gc.presencePenalty !== undefined) {
        requestBody.presence_penalty = gc.presencePenalty;
      }

      // é‡å¤æƒ©ç½š
      if (gc.repetitionPenalty !== undefined) {
        requestBody.repetition_penalty = gc.repetitionPenalty;
      }

      // éšæœºç§å­
      if (gc.seed !== undefined) {
        requestBody.seed = gc.seed;
      }

      // Logit åå·®
      if (gc.logitBias) {
        requestBody.logit_bias = gc.logitBias;
      }
    }

    // ğŸ¾ å¤„ç†è¾“å‡ºé…ç½®ï¼ˆOutputConfigï¼‰
    if (options.outputConfig) {
      const oc = options.outputConfig;

      // æœ€å¤§è¾“å‡º tokens
      if (oc.maxOutputTokens !== undefined) {
        requestBody.max_tokens = oc.maxOutputTokens;
      }

      // è¾“å‡ºæ ¼å¼
      if (oc.outputFormat === "json") {
        requestBody.response_format = { type: "json_object" };
      } else if (oc.outputFormat === "text") {
        requestBody.response_format = { type: "text" };
      }

      // åœæ­¢åºåˆ—
      if (oc.stopSequences && oc.stopSequences.length > 0) {
        requestBody.stop = oc.stopSequences;
      }
    }

    return requestBody;
  }

  async chat(
    messages: Message[],
    options: ChatOptions,
    signal?: AbortSignal
  ): Promise<LLMResponse> {
    const maxRetries = this.config.maxRetries || 3;
    const retryConfig: RetryConfig = {
      maxRetries,
      initialDelay: 1000,
      maxDelay: 10000,
      backoffMultiplier: 2,
      retryOn4xx: false,
      shouldRetry: (error: any) => {
        if (signal?.aborted || error.name === "AbortError" || error.code === "ERR_CANCELED") {
          return false;
        }
        if (
          error.response?.status === 400 ||
          error.response?.status === 401 ||
          error.response?.status === 403 ||
          error.response?.status === 404
        ) {
          return false;
        }
        return true;
      },
    };

    return retry(async () => {
      try {
        const requestBody = this.buildRequestBody(messages, options);

        logger.debug(`[${this.providerName}] Request body`, {
          model: requestBody.model,
          messageCount: messages.length,
        });

        const response = await this.client.post("/chat/completions", requestBody, {
          signal,
        });

        return response.data;
      } catch (error: any) {
        if (signal?.aborted || error.name === "AbortError" || error.code === "ERR_CANCELED") {
          throw error;
        }

        logErrorResponse(this.providerName, error, "chat");
        throw new Error(createErrorMessage(this.providerName, error));
      }
    }, retryConfig);
  }

  async *streamChat(
    messages: Message[],
    options: ChatOptions,
    tools?: any[],
    signal?: AbortSignal
  ): AsyncIterableIterator<string> {
    try {
      const { provider, ...apiOptions } = options;
      const filteredOptions = this.filterOptions(apiOptions);

      // ğŸ¾ å¤„ç†æ¶ˆæ¯æ ¼å¼ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
      const processedMessages = messages.map((msg) => {
        if (Array.isArray(msg.content)) {
          return {
            ...msg,
            content: msg.content.map((part) => {
              if (part.type === "image_url") {
                return {
                  type: "image_url",
                  image_url: part.image_url,
                };
              }
              return {
                type: "text",
                text: part.text || "",
              };
            }),
          };
        }
        return {
          ...msg,
          content: msg.content,
        };
      });

      // ğŸ¾ æ„å»ºåŸºç¡€è¯·æ±‚ä½“ï¼ˆä¸ buildRequestBody ä¿æŒä¸€è‡´ï¼‰
      const requestBody: OpenAIRequestBody = {
        model: options.model || this.config.defaultModel,
        messages: processedMessages,
        stream: true,
        ...filteredOptions,
      };

      // âœ… æ–°å¢ï¼šä¼ é€’ç»™LLMçš„å·¥å…·åˆ—è¡¨
      if (tools && tools.length > 0) {
        requestBody.tools = tools;
        requestBody.tool_choice = "auto";
      }

      // ğŸ¾ å¤„ç†æ¸©åº¦å‚æ•°
      if (options.temperature !== undefined) {
        requestBody.temperature = options.temperature;
      }

      // ğŸ¾ å¤„ç†ç”Ÿæˆé…ç½®
      if (options.generationConfig) {
        const gc = options.generationConfig;
        if (gc.topP !== undefined) requestBody.top_p = gc.topP;
        if (gc.frequencyPenalty !== undefined) requestBody.frequency_penalty = gc.frequencyPenalty;
        if (gc.presencePenalty !== undefined) requestBody.presence_penalty = gc.presencePenalty;
        if (gc.repetitionPenalty !== undefined)
          requestBody.repetition_penalty = gc.repetitionPenalty;
        if (gc.seed !== undefined) requestBody.seed = gc.seed;
        if (gc.logitBias) requestBody.logit_bias = gc.logitBias;
      }

      // ğŸ¾ å¤„ç†è¾“å‡ºé…ç½®
      if (options.outputConfig) {
        const oc = options.outputConfig;
        if (oc.maxOutputTokens !== undefined) requestBody.max_tokens = oc.maxOutputTokens;
        if (oc.outputFormat === "json") {
          requestBody.response_format = { type: "json_object" };
        } else if (oc.outputFormat === "text") {
          requestBody.response_format = { type: "text" };
        }
        if (oc.stopSequences && oc.stopSequences.length > 0) {
          requestBody.stop = oc.stopSequences;
        }
      }

      logger.debug(`[${this.providerName}] Stream request`, {
        model: requestBody.model,
        messageCount: messages.length,
        hasTools: !!tools,
        toolCount: tools?.length,
      });

      const response = await this.client.post("/chat/completions", requestBody, {
        responseType: "stream",
        signal,
      });

      for await (const chunk of response.data) {
        const lines = chunk
          .toString()
          .split("\n")
          .filter((line: string) => line.trim());

        for (const line of lines) {
          if (line.startsWith("data: ")) {
            const data = line.substring(6);

            if (data === "[DONE]") {
              return;
            }

            try {
              const parsed = JSON.parse(data);

              // æå– reasoning_content (æ·±åº¦æ€è€ƒ)
              const reasoning = parsed.choices?.[0]?.delta?.reasoning_content;

              // æå– content (å›ç­”å†…å®¹)
              const content = parsed.choices?.[0]?.delta?.content;

              // æå– tool_calls (å·¥å…·è°ƒç”¨)
              const toolCalls = parsed.choices?.[0]?.delta?.tool_calls;

              // åªè¦æœ‰å†…å®¹å°± yield JSON å­—ç¬¦ä¸²
              if (reasoning || content || toolCalls) {
                yield JSON.stringify({
                  reasoning_content: reasoning,
                  content: content,
                  tool_calls: toolCalls,
                });
              }
            } catch (e) {
              logger.warn(
                `[${this.providerName}] Failed to parse stream chunk:`,
                e instanceof Error ? e.message : String(e)
              );
            }
          }
        }
      }
    } catch (error: any) {
      logger.error(`âŒ ${this.providerName} stream error:`, error.message);
      if (error.response) {
        logger.error(`   HTTPçŠ¶æ€: ${error.response.status}`);
        // ğŸ› ä¿®å¤ï¼šå®‰å…¨åºåˆ—åŒ–ï¼Œé¿å…å¾ªç¯å¼•ç”¨
        try {
          if (error.response.data && typeof error.response.data === "object") {
            // åªåºåˆ—åŒ– data å­—æ®µï¼Œé¿å…åºåˆ—åŒ–æ•´ä¸ª response å¯¹è±¡
            logger.error(`   é”™è¯¯è¯¦æƒ…: ${JSON.stringify(error.response.data, null, 2)}`);
          } else {
            logger.error(`   é”™è¯¯è¯¦æƒ…: ${error.response.data || "æ— è¯¦ç»†ä¿¡æ¯"}`);
          }
        } catch (e) {
          // å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œåªè®°å½•é”™è¯¯æ¶ˆæ¯
          logger.error(`   é”™è¯¯è¯¦æƒ…: [æ— æ³•åºåˆ—åŒ–å“åº”æ•°æ®]`);
        }
      }
      throw new Error(`${this.providerName} stream request failed: ${error.message}`);
    }
  }

  async getModels(): Promise<string[]> {
    try {
      const response = await this.client.get("/models");
      const models = response.data.data || response.data.models || [];
      return models.map((m: any) => m.id || m.name);
    } catch (error: any) {
      logger.warn(`âš ï¸  Failed to get models from ${this.providerName}:`, error.message);
      throw error;
    }
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
   */
  async embed(texts: string[], model?: string): Promise<number[][]> {
    try {
      const requestBody = {
        model: model || this.config.defaultModel,
        input: texts,
      };

      logger.debug(`[${this.providerName}] Embedding request`, {
        model: requestBody.model,
        textCount: texts.length,
      });

      const response = await this.client.post("/embeddings", requestBody);

      // OpenAI æ ¼å¼: { data: [{ embedding: [...] }] }
      if (response.data?.data) {
        return response.data.data.map((item: any) => item.embedding);
      }

      // Ollama æ ¼å¼: { embedding: [...] } æˆ– { embeddings: [[...]] }
      if (response.data?.embedding) {
        return [response.data.embedding];
      }
      if (response.data?.embeddings) {
        return response.data.embeddings;
      }

      throw new Error("Unexpected embedding response format");
    } catch (error: any) {
      logErrorResponse(this.providerName, error, "embed");
      throw new Error(`${this.providerName} embedding failed: ${error.message}`);
    }
  }
}
