/**
 * ApexBridge (ABP-only) - LLMå®¢æˆ·ç«¯
 * ç»Ÿä¸€çš„LLMæä¾›å•†æŠ½è±¡å±‚ï¼Œæ”¯æŒå¤šæä¾›å•†åˆ‡æ¢
 */

import axios, { AxiosInstance } from 'axios';
import { Message, ChatOptions, LLMResponse, LLMProviderConfig, LLMConfig } from '../types';
import { logger } from '../utils/logger';
import { retry, RetryConfig } from '../utils/retry';

/**
 * LLMé€‚é…å™¨æ¥å£
 */
export interface ILLMAdapter {
  chat(messages: Message[], options: ChatOptions, signal?: AbortSignal): Promise<LLMResponse>;
  streamChat(messages: Message[], options: ChatOptions, signal?: AbortSignal): AsyncIterableIterator<string>;
  getModels(): Promise<string[]>;
}

/**
 * OpenAIå…¼å®¹é€‚é…å™¨ï¼ˆé€šç”¨ï¼‰
 * æ”¯æŒï¼šOpenAIã€DeepSeekã€æ™ºè°±ã€Ollamaã€è‡ªå®šä¹‰æä¾›å•†
 */
class OpenAICompatibleAdapter implements ILLMAdapter {
  private client: AxiosInstance;
  
  constructor(
    private providerName: string,
    private config: LLMProviderConfig
  ) {
    this.client = axios.create({
      baseURL: this.config.baseURL,
      headers: {
        ...(this.config.apiKey && { 'Authorization': `Bearer ${this.config.apiKey}` }),
        'Content-Type': 'application/json'
      },
      timeout: this.config.timeout || 60000
    });
    
    logger.info(`âœ… ${providerName} adapter initialized (${this.config.baseURL})`);
  }
  
  async chat(messages: Message[], options: ChatOptions, signal?: AbortSignal): Promise<LLMResponse> {
    // è·å–é‡è¯•é…ç½®ï¼ˆä»é…ç½®æˆ–é»˜è®¤å€¼ï¼‰
    const maxRetries = this.config.maxRetries || 3;
    const retryConfig: RetryConfig = {
      maxRetries,
      initialDelay: 1000,
      maxDelay: 10000,
      backoffMultiplier: 2,
      retryOn4xx: false,
      shouldRetry: (error: any) => {
        // å¦‚æœè¯·æ±‚è¢«ä¸­æ–­ï¼Œä¸é‡è¯•
        if (signal?.aborted || error.name === 'AbortError' || error.code === 'ERR_CANCELED') {
          return false;
        }
        // 400 Bad Request ç­‰å®¢æˆ·ç«¯é”™è¯¯ä¸é‡è¯•
        if (error.response?.status === 400 || error.response?.status === 401 || 
            error.response?.status === 403 || error.response?.status === 404) {
          return false;
        }
        return true;
      }
    };

    return retry(async () => {
      try {
        // ğŸ” æ’é™¤å†…éƒ¨è·¯ç”±å‚æ•°
        const { provider, ...apiOptions } = options;
        
        // ğŸ¯ æ ¹æ®å‚å•†ç‰¹æ€§è¿‡æ»¤å‚æ•°
        let finalOptions = { ...apiOptions };
        
        // DeepSeek ä¸æ”¯æŒ top_k
        if (this.providerName === 'DeepSeek' && 'top_k' in finalOptions) {
          const { top_k, ...rest } = finalOptions;
          finalOptions = rest;
          logger.debug(`[${this.providerName}] Filtered top_k=${top_k}`);
        }
        
        // ğŸ”§ DeepSeeké™åˆ¶ï¼šmax_tokens æœ€å¤§8192
        let maxTokens = options.max_tokens;
        if (this.providerName === 'DeepSeek' && maxTokens && maxTokens > 8192) {
          logger.warn(`âš ï¸  [${this.providerName}] max_tokens ${maxTokens} exceeds limit, capping at 8192`);
          maxTokens = 8192;
        }
        
        const requestBody: any = {
          model: options.model || this.config.defaultModel,
          messages,
          temperature: options.temperature ?? 0.7,
          stream: false,
          ...finalOptions
        };
        
        // åªæ·»åŠ æœ‰å€¼çš„å‚æ•°ï¼Œé¿å…å‘é€ undefined
        if (maxTokens !== undefined) requestBody.max_tokens = maxTokens;
        if (options.top_p !== undefined) requestBody.top_p = options.top_p;
        
        logger.info(`[${this.providerName}] Request body snapshot`, {
          model: requestBody.model,
          hasMessages: Array.isArray(requestBody.messages),
          messageCount: Array.isArray(requestBody.messages) ? requestBody.messages.length : undefined
        });

        const response = await this.client.post('/chat/completions', requestBody, {
          signal
        });
        
        return response.data;
      } catch (error: any) {
        // æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
        if (signal?.aborted || error.name === 'AbortError' || error.code === 'ERR_CANCELED') {
          throw error; // ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
        }

        logger.error(`âŒ ${this.providerName} chat error:`, error.message);
        // ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•400ç­‰é”™è¯¯ï¼‰
        if (error.response) {
          logger.error(`   HTTPçŠ¶æ€: ${error.response.status}`);
          logger.error(`   é”™è¯¯è¯¦æƒ…: ${JSON.stringify(error.response.data)}`);
        }
        throw new Error(`${this.providerName} request failed: ${error.message}`);
      }
    }, retryConfig);
  }
  
  async *streamChat(messages: Message[], options: ChatOptions, signal?: AbortSignal): AsyncIterableIterator<string> {
    try {
      // ğŸ” æ’é™¤å†…éƒ¨è·¯ç”±å‚æ•°
      const { provider, ...apiOptions } = options;
      
      // ğŸ¯ æ ¹æ®å‚å•†ç‰¹æ€§è¿‡æ»¤å‚æ•°
      let finalOptions = { ...apiOptions };
      
      // DeepSeek ä¸æ”¯æŒ top_k
      if (this.providerName === 'DeepSeek' && 'top_k' in finalOptions) {
        const { top_k, ...rest } = finalOptions;
        finalOptions = rest;
        logger.debug(`[${this.providerName}] Stream filtered top_k=${top_k}`);
      }
      
      // ğŸ”§ DeepSeeké™åˆ¶ï¼šmax_tokens æœ€å¤§8192
      let maxTokens = options.max_tokens;
      if (this.providerName === 'DeepSeek' && maxTokens && maxTokens > 8192) {
        logger.warn(`âš ï¸  [${this.providerName}] max_tokens ${maxTokens} exceeds limit, capping at 8192`);
        maxTokens = 8192;
      }
      
      const requestBody: any = {
        model: options.model || this.config.defaultModel,
        messages,
        temperature: options.temperature ?? 0.7,
        stream: true,
        ...finalOptions
      };
      
      // åªæ·»åŠ æœ‰å€¼çš„å‚æ•°ï¼Œé¿å…å‘é€ undefined
      if (maxTokens !== undefined) requestBody.max_tokens = maxTokens;
      if (options.top_p !== undefined) requestBody.top_p = options.top_p;
      
      logger.info(`[${this.providerName}] Stream request body snapshot`, {
        model: requestBody.model,
        hasMessages: Array.isArray(requestBody.messages),
        messageCount: Array.isArray(requestBody.messages) ? requestBody.messages.length : undefined
      });

      // ğŸ†• æ·»åŠ  AbortSignal æ”¯æŒ
      const response = await this.client.post('/chat/completions', requestBody, {
        responseType: 'stream',
        signal: signal  // ä¼ é€’ä¸­æ–­ä¿¡å·
      });
      
      for await (const chunk of response.data) {
        const lines = chunk.toString().split('\n').filter((line: string) => line.trim());
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.substring(6);
            
            if (data === '[DONE]') {
              return;
            }
            
            try {
              const parsed = JSON.parse(data);
              const content = parsed.choices?.[0]?.delta?.content;
              
              if (content) {
                yield content;
              }
            } catch (e) {
              // Skip parse errors
            }
          }
        }
      }
    } catch (error: any) {
      logger.error(`âŒ ${this.providerName} stream error:`, error.message);
      // ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•400ç­‰é”™è¯¯ï¼‰
      if (error.response) {
        logger.error(`   HTTPçŠ¶æ€: ${error.response.status}`);
        logger.error(`   é”™è¯¯è¯¦æƒ…: ${JSON.stringify(error.response.data)}`);
      }
      throw new Error(`${this.providerName} stream request failed: ${error.message}`);
    }
  }
  
  async getModels(): Promise<string[]> {
    try {
      const response = await this.client.get('/models');
      const models = response.data.data || response.data.models || [];
      return models.map((m: any) => m.id || m.name);
    } catch (error: any) {
      logger.warn(`âš ï¸  Failed to get models from ${this.providerName}:`, error.message);
      // è¿”å›é»˜è®¤æ¨¡å‹
      return [this.config.defaultModel];
    }
  }
}

/**
 * Claudeé€‚é…å™¨ï¼ˆAnthropic APIï¼‰
 */
class ClaudeAdapter implements ILLMAdapter {
  private client: AxiosInstance;
  
  constructor(private config: LLMProviderConfig) {
    this.client = axios.create({
      baseURL: this.config.baseURL,
      headers: {
        'x-api-key': this.config.apiKey || '',
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json'
      },
      timeout: this.config.timeout || 60000
    });
    
    logger.info(`âœ… Claude adapter initialized (${this.config.baseURL})`);
  }
  
  async chat(messages: Message[], options: ChatOptions, signal?: AbortSignal): Promise<LLMResponse> {
    // è·å–é‡è¯•é…ç½®
    const maxRetries = this.config.maxRetries || 3;
    const retryConfig: RetryConfig = {
      maxRetries,
      initialDelay: 1000,
      maxDelay: 10000,
      backoffMultiplier: 2,
      retryOn4xx: false,
      shouldRetry: (error: any) => {
        // å¦‚æœè¯·æ±‚è¢«ä¸­æ–­ï¼Œä¸é‡è¯•
        if (signal?.aborted || error.name === 'AbortError' || error.code === 'ERR_CANCELED') {
          return false;
        }
        // 400 Bad Request ç­‰å®¢æˆ·ç«¯é”™è¯¯ä¸é‡è¯•
        if (error.response?.status === 400 || error.response?.status === 401 || 
            error.response?.status === 403 || error.response?.status === 404) {
          return false;
        }
        return true;
      }
    };

    return retry(async () => {
      try {
        // è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼šåˆ†ç¦»systemå’Œå…¶ä»–æ¶ˆæ¯
        const systemMessages = messages.filter(m => m.role === 'system');
        const otherMessages = messages.filter(m => m.role !== 'system');
        
        const response = await this.client.post('/messages', {
          model: options.model || this.config.defaultModel,
          max_tokens: options.max_tokens || 4096,
          temperature: options.temperature ?? 0.7,
          system: systemMessages.map(m => m.content).join('\n\n') || undefined,
          messages: otherMessages.map(m => ({
            role: m.role === 'assistant' ? 'assistant' : 'user',
            content: m.content
          })),
          stream: false
        }, {
          signal
        });
        
        // è½¬æ¢ä¸ºOpenAIæ ¼å¼
        return {
          id: response.data.id,
          object: 'chat.completion',
          created: Math.floor(Date.now() / 1000),
          model: response.data.model,
          choices: [{
            index: 0,
            message: {
              role: 'assistant',
              content: response.data.content[0]?.text || ''
            },
            finish_reason: response.data.stop_reason
          }]
        };
      } catch (error: any) {
        // æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
        if (signal?.aborted || error.name === 'AbortError' || error.code === 'ERR_CANCELED') {
          throw error; // ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
        }

        logger.error('âŒ Claude chat error:', error.message);
        throw new Error(`Claude request failed: ${error.message}`);
      }
    }, retryConfig);
  }
  
  async *streamChat(messages: Message[], options: ChatOptions, signal?: AbortSignal): AsyncIterableIterator<string> {
    try {
      const systemMessages = messages.filter(m => m.role === 'system');
      const otherMessages = messages.filter(m => m.role !== 'system');
      
      // ğŸ†• æ·»åŠ  AbortSignal æ”¯æŒ
      const response = await this.client.post('/messages', {
        model: options.model || this.config.defaultModel,
        max_tokens: options.max_tokens || 4096,
        temperature: options.temperature ?? 0.7,
        system: systemMessages.map(m => m.content).join('\n\n') || undefined,
        messages: otherMessages.map(m => ({
          role: m.role === 'assistant' ? 'assistant' : 'user',
          content: m.content
        })),
        stream: true
      }, {
        responseType: 'stream',
        signal: signal  // ä¼ é€’ä¸­æ–­ä¿¡å·
      });
      
      for await (const chunk of response.data) {
        const lines = chunk.toString().split('\n').filter((line: string) => line.trim());
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.substring(6);
            
            try {
              const parsed = JSON.parse(data);
              
              if (parsed.type === 'content_block_delta') {
                const content = parsed.delta?.text;
                if (content) {
                  yield content;
                }
              }
            } catch (e) {
              // Skip parse errors
            }
          }
        }
      }
    } catch (error: any) {
      logger.error('âŒ Claude stream error:', error.message);
      throw new Error(`Claude stream request failed: ${error.message}`);
    }
  }
  
  async getModels(): Promise<string[]> {
    // Claudeä¸æä¾›æ¨¡å‹åˆ—è¡¨APIï¼Œè¿”å›å¸¸ç”¨æ¨¡å‹
    return [
      'claude-3-5-sonnet-20241022',
      'claude-3-5-haiku-20241022',
      'claude-3-opus-20240229',
      'claude-3-sonnet-20240229',
      'claude-3-haiku-20240307'
    ];
  }
}

/**
 * LLMå®¢æˆ·ç«¯
 */
export class LLMClient {
  private adapters: Map<string, ILLMAdapter> = new Map();
  private defaultProvider: string | null = null;
  
  constructor(config: LLMConfig) {
    logger.info('ğŸ¤– Initializing LLM Client with multiple providers...');
    
    // æ³¨å†ŒOpenAI
    if (config.openai) {
      this.adapters.set('openai', new OpenAICompatibleAdapter('OpenAI', config.openai));
    }
    
    // æ³¨å†ŒDeepSeek
    if (config.deepseek) {
      this.adapters.set('deepseek', new OpenAICompatibleAdapter('DeepSeek', config.deepseek));
    }
    
    // æ³¨å†Œæ™ºè°±AI
    if (config.zhipu) {
      this.adapters.set('zhipu', new OpenAICompatibleAdapter('ZhipuAI', config.zhipu));
    }
    
    // æ³¨å†ŒClaude
    if (config.claude) {
      this.adapters.set('claude', new ClaudeAdapter(config.claude));
    }
    
    // æ³¨å†ŒOllama
    if (config.ollama) {
      this.adapters.set('ollama', new OpenAICompatibleAdapter('Ollama', config.ollama));
    }
    
    // æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
    if (config.custom) {
      this.adapters.set('custom', new OpenAICompatibleAdapter('Custom', config.custom));
    }
    
    // è®¾ç½®é»˜è®¤æä¾›å•†
    if (config.defaultProvider && this.adapters.has(config.defaultProvider)) {
      this.defaultProvider = config.defaultProvider;
    } else {
      // è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
      this.defaultProvider = Array.from(this.adapters.keys())[0] || null;
    }
    
    if (!this.defaultProvider) {
      throw new Error('No LLM providers configured');
    }
    
    logger.info(`âœ… LLM Client initialized with ${this.adapters.size} providers`);
    logger.info(`ğŸ“Œ Default provider: ${this.defaultProvider}`);
    logger.info(`ğŸ“‹ Available providers: ${Array.from(this.adapters.keys()).join(', ')}`);
  }
  
  /**
   * æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ£€æµ‹æä¾›å•†
   */
  private detectProvider(model?: string): string {
    if (!model) {
      return this.defaultProvider!;
    }
    
    // æ ¹æ®æ¨¡å‹åç§°å‰ç¼€åˆ¤æ–­
    if (model.startsWith('gpt-')) return 'openai';
    if (model.startsWith('deepseek-')) return 'deepseek';
    if (model.startsWith('glm-')) return 'zhipu';
    if (model.startsWith('claude-')) return 'claude';
    if (model.startsWith('llama') || model.startsWith('qwen') || model.startsWith('mistral')) return 'ollama';
    
    // å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œä½¿ç”¨é»˜è®¤æä¾›å•†
    return this.defaultProvider!;
  }
  
  /**
   * è·å–æŒ‡å®šæä¾›å•†çš„é€‚é…å™¨
   */
  private getAdapter(provider: string): ILLMAdapter {
    const adapter = this.adapters.get(provider);
    
    if (!adapter) {
      throw new Error(`LLM provider '${provider}' not configured. Available: ${Array.from(this.adapters.keys()).join(', ')}`);
    }
    
    return adapter;
  }
  
  async chat(messages: Message[], options: ChatOptions = {}): Promise<LLMResponse> {
    const provider = options.provider || this.detectProvider(options.model);
    const adapter = this.getAdapter(provider);
    
    logger.debug(`ğŸ’¬ Calling LLM: ${provider}, model: ${options.model || 'default'}`);
    
    return await adapter.chat(messages, options);
  }
  
  async *streamChat(messages: Message[], options: ChatOptions = {}, signal?: AbortSignal): AsyncIterableIterator<string> {
    const provider = options.provider || this.detectProvider(options.model);
    const adapter = this.getAdapter(provider);
    
    logger.debug(`ğŸŒŠ Streaming from LLM: ${provider}, model: ${options.model || 'default'}`);
    
    // ğŸ†• ä¼ é€’ä¸­æ–­ä¿¡å·
    yield* adapter.streamChat(messages, options, signal);
  }
  
  async getAllModels(): Promise<Array<{ id: string; provider: string }>> {
    const models: Array<{ id: string; provider: string }> = [];
    
    for (const [provider, adapter] of this.adapters) {
      try {
        const providerModels = await adapter.getModels();
        models.push(...providerModels.map(id => ({ id, provider })));
      } catch (error: any) {
        logger.warn(`âš ï¸  Failed to get models from ${provider}:`, error.message);
      }
    }
    
    return models;
  }
  
  /**
   * è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨
   */
  getAvailableProviders(): string[] {
    return Array.from(this.adapters.keys());
  }
  
  /**
   * è·å–é»˜è®¤æä¾›å•†
   */
  getDefaultProvider(): string | null {
    return this.defaultProvider;
  }
}
