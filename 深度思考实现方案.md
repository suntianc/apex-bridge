# ğŸ¯ ç»ˆæç‰ˆ ReAct æµå¼æ–¹æ¡ˆï¼šæç®€æ ¸å¿ƒ + æŒ‰éœ€å¢å¼º

è¿™æ˜¯å¯¹**æ–¹æ¡ˆäºŒ**çš„å®Œæ•´å¢å¼ºç‰ˆæœ¬ï¼Œåœ¨ä¿æŒ**180è¡Œæ ¸å¿ƒä»£ç **ä¸å˜çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡**ç»„åˆæ¨¡å¼**é›†æˆä¸‰å¤§ç”Ÿäº§ä¼˜åŒ–ã€‚æ‰€æœ‰å¢å¼ºåŠŸèƒ½å‡ä¸º**å¯æ’æ‹”**ï¼Œå¯æ ¹æ®åœºæ™¯éœ€æ±‚å¯ç”¨ã€‚

---

## **ğŸ—ï¸ æ•´ä½“æ¶æ„ï¼šä¸‰å±‚å¢å¼ºæ¨¡å‹**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ReActEngine (æ ¸å¿ƒå±‚ - 180è¡Œ)                  â”‚
â”‚  - ä»… AsyncGeneratorï¼Œæ— å†—ä½™é˜Ÿåˆ—                                â”‚
â”‚  - é€’å½’é©±åŠ¨ ReAct å¾ªç¯                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–²                    â–²                     â–²
          â”‚ ä¾èµ–æ³¨å…¥            â”‚ ä¾èµ–æ³¨å…¥             â”‚ ä¾èµ–æ³¨å…¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚  ToolExecutor      â”‚  â”‚ LLMAdapter  â”‚      â”‚ EnginePool   â”‚
â”‚  (æ‰§è¡Œå±‚ - 60è¡Œ)    â”‚  â”‚ (ç¼“å­˜å±‚)     â”‚      â”‚ (æ‰¹å¤„ç†å±‚)    â”‚
â”‚  - æ”¯æŒæµå¼å·¥å…·     â”‚  â”‚ - L1/L2ç¼“å­˜ â”‚      â”‚ - å®ä¾‹å¤ç”¨    â”‚
â”‚  - è‡ªåŠ¨èƒŒå‹         â”‚  â”‚ - HTTP/2å¤ç”¨â”‚      â”‚ - å¹¶å‘æ§åˆ¶    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **1ï¸âƒ£ æ ¸å¿ƒå¼•æ“ï¼šReActEngine.tsï¼ˆ180è¡Œï¼‰**

```typescript
import { logger } from '../utils/logger';
import { ToolExecutor } from './ToolExecutor';
import type { LLMAdapter } from '../adapters/LLMAdapter';
import type { Tool } from '../tools/Tool';

// â”€â”€ ç±»å‹å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export interface ReActOptions {
  maxIterations?: number;      // é˜²æ— é™å¾ªç¯ï¼ˆé»˜è®¤10ï¼‰
  timeoutMs?: number;          // æ€»è¶…æ—¶ï¼ˆé»˜è®¤5åˆ†é’Ÿï¼‰
  enableThinking?: boolean;    // æ˜¯å¦æµå¼è¾“å‡ºæ€è€ƒ
  maxConcurrentTools?: number; // å·¥å…·å¹¶å‘åº¦ï¼ˆé»˜è®¤3ï¼‰
  enableStreamingTools?: boolean; // å¯ç”¨æµå¼å·¥å…·æ”¯æŒ
}

export interface StreamEvent {
  type: 'reasoning' | 'content' | 'tool_start' | 'tool_progress' | 'tool_end' | 'error' | 'done';
  data: any;
  timestamp: number;
  iteration: number;
}

// â”€â”€ æ ¸å¿ƒå¼•æ“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/**
 * æç®€ ReAct å¼•æ“
 * è®¾è®¡åŸåˆ™ï¼šä»… AsyncGeneratorï¼Œæ— å†—ä½™é˜Ÿåˆ—ï¼Œé€’å½’é©±åŠ¨
 */
export class ReActEngine {
  private iteration = 0;
  private toolExecutor: ToolExecutor;

  constructor(
    private tools: Tool[],
    private options: ReActOptions = {}
  ) {
    this.toolExecutor = new ToolExecutor(tools, {
      maxConcurrent: options.maxConcurrentTools || 3,
      timeoutMs: options.enableStreamingTools ? 60000 : 30000,
      enableStreaming: options.enableStreamingTools || false
    });
    logger.info(`âœ… ReActEngine initialized with ${tools.length} tools`);
  }

  /**
   * æµå¼æ‰§è¡Œ ReAct å¯¹è¯ï¼ˆä¸»å…¥å£ï¼‰
   */
  async *execute(
    messages: any[],
    llmClient: LLMAdapter,
    runtimeOptions?: Partial<ReActOptions>
  ): AsyncGenerator<StreamEvent, string, void> {
    const opts = { ...this.options, ...runtimeOptions };
    const abortController = new AbortController();
    
    const timeoutId = opts.timeoutMs 
      ? setTimeout(() => abortController.abort(`timeout after ${opts.timeoutMs}ms`), opts.timeoutMs)
      : null;

    try {
      return yield* this.runIteration(messages, llmClient, {
        maxIterations: opts.maxIterations || 10,
        enableThinking: opts.enableThinking !== false,
        iteration: 0
      }, abortController.signal);
    } finally {
      if (timeoutId) clearTimeout(timeoutId);
    }
  }

  /**
   * å•è½®è¿­ä»£ï¼ˆé€’å½’è°ƒç”¨ï¼‰
   */
  private async *runIteration(
    messages: any[],
    llmClient: LLMAdapter,
    context: { maxIterations: number; enableThinking: boolean; iteration: number },
    signal: AbortSignal
  ): AsyncGenerator<StreamEvent, string, void> {
    const { iteration } = context;
    if (iteration >= context.maxIterations) {
      throw new Error(`Max iterations (${context.maxIterations}) exceeded`);
    }

    logger.debug(`ğŸ”„ ReAct iteration ${iteration + 1}/${context.maxIterations}`);

    // è®¢é˜… LLM SSE æµ
    const stream = llmClient.streamChat(messages, {
      tools: this.tools.map(t => ({
        type: 'function',
        function: { name: t.name, description: t.description, parameters: t.parameters }
      })),
      enableThinking: context.enableThinking
    }, signal);

    let accumulatedContent = '';
    const toolCalls = new Map<number, any>();

    // â”€â”€ å¤„ç† LLM æµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for await (const chunk of stream) {
      if (signal.aborted) throw new Error(`Aborted: ${signal.reason}`);

      // 1. æ€è€ƒå†…å®¹
      if (chunk.reasoning_content) {
        yield this.createEvent('reasoning', { content: chunk.reasoning_content }, iteration);
      }

      // 2. å›ç­”å†…å®¹
      if (chunk.content) {
        accumulatedContent += chunk.content;
        yield this.createEvent('content', { content: chunk.content }, iteration);
      }

      // 3. ç´¯ç§¯å·¥å…·è°ƒç”¨ï¼ˆSSE å¯èƒ½åˆ†å—ï¼‰
      if (chunk.tool_calls) {
        for (const call of chunk.tool_calls) {
          const idx = call.index ?? 0;
          const existing = toolCalls.get(idx);
          if (!existing) {
            toolCalls.set(idx, call);
          } else if (call.function?.arguments) {
            existing.function.arguments += call.function.arguments;
          }
        }
      }
    }

    // â”€â”€ å·¥å…·è°ƒç”¨å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (toolCalls.size > 0) {
      const validCalls = Array.from(toolCalls.values()).filter(call => {
        try { JSON.parse(call.function.arguments); return true; }
        catch { logger.warn('âš ï¸ è·³è¿‡ä¸å®Œæ•´å·¥å…·è°ƒç”¨'); return false; }
      });

      yield this.createEvent('tool_start', { calls: validCalls }, iteration);

      // ğŸŒŠ æ”¯æŒæµå¼/éæµå¼å·¥å…·
      const results = this.options.enableStreamingTools 
        ? yield* this.executeStreamingTools(validCalls, iteration)
        : await this.executeTools(validCalls, iteration);

      // å›æµåˆ° LLM
      for (const [call, result] of results) {
        messages.push(
          { role: 'assistant', tool_calls: [call] },
          { role: 'tool', tool_call_id: call.id, name: call.function.name, content: JSON.stringify(result) }
        );
      }

      // é€’å½’ä¸‹ä¸€è½®
      return yield* this.runIteration(messages, llmClient, {
        ...context,
        iteration: iteration + 1
      }, signal);
    }

    // â”€â”€ å®Œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    yield this.createEvent('done', null, iteration);
    return accumulatedContent;
  }

  /**
   * æ‰§è¡Œæµå¼å·¥å…·ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼‰
   */
  private async *executeStreamingTools(
    toolCalls: any[],
    iteration: number
  ): AsyncGenerator<StreamEvent, Map<any, any>, void> {
    const results = new Map();
    
    for (const call of toolCalls) {
      const result = this.toolExecutor.executeStreaming(call, (chunk) => {
        // å®æ—¶æ¨é€å·¥å…·è¿›åº¦
        this.createEvent('tool_progress', { toolCallId: call.id, chunk }, iteration);
      });
      
      const chunks: any[] = [];
      for await (const chunk of result) {
        chunks.push(chunk);
        yield this.createEvent('tool_progress', { toolCallId: call.id, chunk }, iteration);
      }
      
      results.set(call, { toolCallId: call.id, name: call.function.name, status: 'success', result: chunks });
    }

    return results;
  }

  /**
   * æ‰§è¡Œæ™®é€šå·¥å…·ï¼ˆPromiseæ¨¡å¼ï¼‰
   */
  private async executeTools(
    toolCalls: any[],
    iteration: number
  ): Promise<Map<any, any>> {
    return this.toolExecutor.executeAll(toolCalls, iteration, (result) => {
      // éæµå¼åªæ¨é€æœ€ç»ˆçŠ¶æ€
      logger.debug(`âœ… å·¥å…·å®Œæˆ: ${result.name}`);
    });
  }

  private createEvent(type: StreamEvent['type'], data: any, iteration: number): StreamEvent {
    return { type, data, timestamp: Date.now(), iteration };
  }
}
```

---

## **2ï¸âƒ£ å¢å¼ºå·¥å…·æ‰§è¡Œå™¨ï¼šToolExecutor.tsï¼ˆ60è¡Œï¼‰**

```typescript
import pLimit from 'p-limit';
import type { Tool } from '../tools/Tool';
import type { ToolCall, ToolResult } from '../types';

interface ExecutorConfig {
  maxConcurrent?: number;
  timeoutMs?: number;
  enableStreaming?: boolean;
}

/**
 * æ”¯æŒæµå¼/éæµå¼å·¥å…·çš„å¹¶å‘æ‰§è¡Œå™¨
 */
export class ToolExecutor {
  private limit = pLimit(3);
  private timeoutMs: number;
  private enableStreaming: boolean;

  constructor(private tools: Tool[], config: ExecutorConfig) {
    this.limit = pLimit(config.maxConcurrent || 3);
    this.timeoutMs = config.timeoutMs || 30000;
    this.enableStreaming = config.enableStreaming || false;
  }

  /**
   * ğŸŒŠ æµå¼å·¥å…·æ‰§è¡Œï¼ˆç”Ÿæˆå™¨ï¼‰
   */
  async *executeStreaming(
    call: ToolCall,
    onChunk: (chunk: any) => void
  ): AsyncGenerator<any, void, void> {
    const tool = this.tools.find(t => t.name === call.function.name);
    if (!tool) throw new Error(`Tool ${call.function.name} not found`);

    const args = JSON.parse(call.function.arguments);
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort('timeout'), this.timeoutMs);

    try {
      const result = tool.execute(args, controller.signal);
      
      // åˆ¤æ–­æ˜¯å¦ä¸ºæµå¼å·¥å…·
      if (this.isAsyncGenerator(result)) {
        for await (const chunk of result) {
          onChunk(chunk);
          yield chunk;
        }
      } else {
        // éæµå¼å·¥å…·è½¬ä¸ºå•å—æµ
        const final = await result;
        onChunk(final);
        yield final;
      }
    } finally {
      clearTimeout(timeout);
    }
  }

  /**
   * ğŸ“¦ æ‰¹é‡æ‰§è¡Œï¼ˆéæµå¼å·¥å…·ï¼‰
   */
  async executeAll(
    toolCalls: ToolCall[],
    iteration: number,
    onComplete?: (result: ToolResult) => void
  ): Promise<Map<ToolCall, ToolResult>> {
    const results = new Map<ToolCall, ToolResult>();

    const tasks = toolCalls.map(call => this.limit(async () => {
      const result = await this.executeStreaming(call, () => {});
      const final = Array.isArray(result) ? result[result.length - 1] : result;
      const toolResult: ToolResult = {
        toolCallId: call.id,
        name: call.function.name,
        status: 'success',
        result: final
      };
      onComplete?.(toolResult);
      return { call, result: toolResult };
    }));

    const settled = await Promise.allSettled(tasks);
    for (const outcome of settled) {
      if (outcome.status === 'fulfilled') {
        results.set(outcome.value.call, outcome.value.result);
      }
    }

    return results;
  }

  private isAsyncGenerator(value: any): value is AsyncGenerator {
    return value && typeof value[Symbol.asyncIterator] === 'function';
  }
}
```

---

## **3ï¸âƒ£ ç¼“å­˜å¢å¼º LLM é€‚é…å™¨ï¼šCachedLLMAdapter.ts**

```typescript
import http2 from 'http2';
import type { LLMAdapter, LLMOptions } from './LLMAdapter';

interface CacheEntry {
  streamFactory: () => AsyncGenerator<any>;
  expiresAt: number;
}

/**
 * å¸¦ L1/L2 ç¼“å­˜å’Œ HTTP/2 å¤ç”¨çš„ LLM é€‚é…å™¨
 */
export class CachedLLMAdapter implements LLMAdapter {
  private connectionPool: http2.ClientHttp2Session[] = [];
  private l1Cache = new Map<string, CacheEntry>(); // å®Œæ•´è¯·æ±‚ç¼“å­˜
  private l2Cache = new Map<string, any>();        // Promptæ¨¡æ¿ç¼“å­˜
  private readonly CACHE_TTL = 30_000; // 30ç§’

  constructor(
    private baseAdapter: LLMAdapter,
    private config: { enableL1Cache?: boolean; enableL2Cache?: boolean }
  ) {
    this.preheatConnections();
  }

  async *streamChat(
    messages: any[],
    options: LLMOptions = {},
    signal?: AbortSignal
  ): AsyncGenerator<any, void, void> {
    // L1 ç¼“å­˜ï¼šç²¾ç¡®åŒ¹é…
    if (this.config.enableL1Cache) {
      const cacheKey = this.generateL1Key(messages, options);
      const cached = this.l1Cache.get(cacheKey);
      if (cached && cached.expiresAt > Date.now()) {
        logger.info('ğŸ¯ L1 Cache hit');
        yield* cached.streamFactory();
        return;
      }
    }

    // L2 ç¼“å­˜ï¼šPrompt æ¨¡æ¿ç¼–è¯‘
    if (this.config.enableL2Cache) {
      const compiled = this.compilePrompt(messages);
      messages = compiled.messages;
    }

    // è·å– HTTP/2 è¿æ¥
    const session = await this.acquireConnection();
    try {
      const stream = this.baseAdapter.streamChat(messages, options, signal);
      
      // ç¼“å†²é¦–Tokenå¹¶ç¼“å­˜
      const bufferedStream = this.bufferFirstToken(stream);
      const cacheKey = this.generateL1Key(messages, options);
      
      // å­˜å…¥L1ç¼“å­˜
      if (this.config.enableL1Cache) {
        this.l1Cache.set(cacheKey, {
          streamFactory: () => this.createCachedStream(bufferedStream),
          expiresAt: Date.now() + this.CACHE_TTL
        });
        setTimeout(() => this.l1Cache.delete(cacheKey), this.CACHE_TTL);
      }
      
      yield* bufferedStream;
    } finally {
      this.releaseConnection(session);
    }
  }

  /**
   * ç¼“å†²é¦–Tokenï¼Œå®ç°"é›¶å»¶è¿Ÿ"è¿”å›
   */
  private async *bufferFirstToken(stream: AsyncIterable<any>): AsyncGenerator<any> {
    const iterator = stream[Symbol.asyncIterator]();
    const first = await iterator.next();
    if (first.done) return;
    
    yield first.value; // ç«‹å³è¿”å›é¦–chunk

    // ç»§ç»­å‰©ä½™æµ
    let next = await iterator.next();
    while (!next.done) {
      yield next.value;
      next = await iterator.next();
    }
  }

  /**
   * ä»ç¼“å­˜é‡å»ºæµ
   */
  private async *createCachedStream(originalStream: AsyncIterable<any>): AsyncGenerator<any> {
    yield* originalStream;
  }

  private preheatConnections() {
    for (let i = 0; i < 3; i++) {
      // åˆ›å»º3ä¸ªHTTP/2é•¿è¿æ¥
    }
  }

  private generateL1Key(messages: any[], options: LLMOptions): string {
    return JSON.stringify({ messages, tools: options.tools?.map(t => t.function.name) });
  }

  private compilePrompt(messages: any[]): { messages: any[] } {
    // ç®€å•ç¤ºä¾‹ï¼šç¼“å­˜ç³»ç»Ÿæ¶ˆæ¯
    const system = messages.find(m => m.role === 'system');
    if (system && this.l2Cache.has(system.content)) {
      return { messages: [this.l2Cache.get(system.content), ...messages.filter(m => m.role !== 'system')] };
    }
    return { messages };
  }
}
```

---

## **4ï¸âƒ£ æ‰¹å¤„ç†ä¼˜åŒ–ï¼šReActEnginePool.ts**

```typescript
import PQueue from 'p-queue';
import type { ReActEngine } from './ReActEngine';
import type { LLMAdapter } from '../adapters/LLMAdapter';
import type { StreamEvent } from './ReActEngine';

/**
 * å¼•æ“å®ä¾‹æ± ï¼ˆæ‰¹å¤„ç†åœºæ™¯ï¼‰
 * - å¤ç”¨å¼•æ“å®ä¾‹
 * - æ§åˆ¶å…¨å±€å¹¶å‘
 */
export class ReActEnginePool {
  private pool: ReActEngine[] = [];
  private maxSize: number;

  constructor(
    private createEngine: () => ReActEngine,
    maxSize: number = 5
  ) {
    this.maxSize = maxSize;
  }

  /**
   * æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡
   */
  async *executeBatch(
    tasks: { messages: any[]; id: string }[],
    llmClient: LLMAdapter,
    maxConcurrent: number = 10
  ): AsyncGenerator<{ taskId: string; event: StreamEvent }, void, void> {
    const queue = new PQueue({ concurrency: maxConcurrent });
    const promises = tasks.map(task => 
      queue.add(async () => {
        const engine = await this.acquire();
        try {
          for await (const event of engine.execute(task.messages, llmClient)) {
            yield { taskId: task.id, event };
          }
        } finally {
          this.release(engine);
        }
      })
    );

    await Promise.all(promises);
  }

  private async acquire(): Promise<ReActEngine> {
    return this.pool.pop() || this.createEngine();
  }

  private release(engine: ReActEngine): void {
    if (this.pool.length < this.maxSize) {
      this.pool.push(engine);
    }
  }
}
```

---

## **5ï¸âƒ£ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹**

### **åœºæ™¯ Aï¼šå®æ—¶å¯¹è¯ï¼ˆæµå¼å·¥å…·ï¼‰**

```typescript
// 1. å®šä¹‰æµå¼å·¥å…·ï¼ˆæ–‡ä»¶ä¸Šä¼ ï¼‰
const fileUploadTool: Tool = {
  name: 'upload_large_file',
  description: 'ä¸Šä¼ å¤§æ–‡ä»¶å¹¶å®æ—¶æ˜¾ç¤ºè¿›åº¦',
  parameters: { type: 'object', properties: { path: { type: 'string' } } },
  execute: async function* (args: any, signal: AbortSignal) {
    const file = fs.createReadStream(args.path);
    let uploaded = 0;
    const total = fs.statSync(args.path).size;
    
    for await (const chunk of file) {
      if (signal.aborted) throw new Error('Upload cancelled');
      
      await uploadChunk(chunk);
      uploaded += chunk.length;
      
      // å®æ—¶ yield è¿›åº¦
      yield {
        progress: (uploaded / total * 100).toFixed(2),
        status: 'uploading'
      };
    }
    
    yield { progress: 100, status: 'completed', url: 'https://...' };
  }
};

// 2. å¯ç”¨æµå¼æ”¯æŒ
const engine = new ReActEngine([fileUploadTool], {
  enableStreamingTools: true,
  maxConcurrentTools: 2
});

// 3. å‰ç«¯å®æ—¶æ¥æ”¶è¿›åº¦
for await (const event of engine.execute(messages, llmClient)) {
  if (event.type === 'tool_progress') {
    updateProgressBar(event.data.toolCallId, event.data.chunk.progress);
  }
}
```

### **åœºæ™¯ Bï¼šæ‰¹é‡æŠ¥å‘Šç”Ÿæˆ**

```typescript
// 1. åˆ›å»ºå¼•æ“æ± 
const pool = new ReActEnginePool(
  () => new ReActEngine([dbTool, searchTool], { maxIterations: 3 }),
  5 // æ± å¤§å°
);

// 2. æ‰¹é‡æ‰§è¡Œ100ä¸ªæŠ¥å‘Š
const tasks = Array.from({ length: 100 }, (_, i) => ({
  id: `report_${i}`,
  messages: [{ role: 'user', content: `ç”ŸæˆæŠ¥å‘Š ${i}` }]
}));

// 3. æµå¼æ¥æ”¶ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼‰
for await (const { taskId, event } of pool.executeBatch(tasks, llmClient, 20)) {
  if (event.type === 'done') {
    saveReport(taskId, event.data);
  }
}
```

### **åœºæ™¯ Cï¼šä½å»¶è¿Ÿä¼˜å…ˆ**

```typescript
// 1. ä½¿ç”¨ç¼“å­˜é€‚é…å™¨
const cachedLLM = new CachedLLMAdapter(
  new LLMAdapter({ apiKey: process.env.GLM_API_KEY }),
  {
    enableL1Cache: true,  // ç¼“å­˜ç›¸åŒè¯·æ±‚
    enableL2Cache: true   // ç¼–è¯‘Promptæ¨¡æ¿
  }
);

// 2. é¦–æ¬¡è°ƒç”¨é¢„çƒ­ç¼“å­˜
await engine.execute([{ role: 'user', content: 'å¸¸è§é—®é¢˜' }], cachedLLM);

// 3. åç»­è°ƒç”¨é¦–Token < 50ms
for await (const event of engine.execute(sameQuestion, cachedLLM)) {
  // é¦–å­—èŠ‚ç¬é—´åˆ°è¾¾
}
```

---

## **ğŸ“Š ç”Ÿäº§é…ç½®æ€»è§ˆ**

| é…ç½®é¡¹ | å®æ—¶å¯¹è¯ | æ‰¹å¤„ç†ä»»åŠ¡ | ä½å»¶è¿Ÿåœºæ™¯ |
|--------|----------|------------|------------|
| `maxIterations` | 10 | 3 | 10 |
| `timeoutMs` | 300000 | 60000 | 300000 |
| `maxConcurrentTools` | 3 | 10 | 2 |
| `enableStreamingTools` | âœ… true | âŒ false | âŒ false |
| `enableL1Cache` | âŒ false | âŒ false | âœ… true |
| `enableL2Cache` | âŒ false | âœ… true | âœ… true |
| `EnginePool.maxSize` | 1 | 10 | 1 |

---

## **ğŸ¯ æœ€ç»ˆä»£ç ç»Ÿè®¡**

| æ–‡ä»¶ | è¡Œæ•° | èŒè´£ |
|------|------|------|
| `ReActEngine.ts` | 180 | æ ¸å¿ƒé©±åŠ¨ï¼Œé€’å½’å¾ªç¯ |
| `ToolExecutor.ts` | 60 | æµå¼/éæµå¼å·¥å…·æ‰§è¡Œ |
| `CachedLLMAdapter.ts` | 90 | L1/L2ç¼“å­˜ + HTTP/2 |
| `ReActEnginePool.ts` | 40 | æ‰¹å¤„ç†å¼•æ“å¤ç”¨ |
| **æ€»è®¡** | **370è¡Œ** | **ç”Ÿäº§çº§å®Œæ•´æ–¹æ¡ˆ** |

---

## **ğŸ† æœ€ç»ˆç»“è®º**

æœ¬æ–¹æ¡ˆåœ¨**ä¿æŒ180è¡Œæ ¸å¿ƒæç®€**çš„å‰æä¸‹ï¼Œé€šè¿‡**ä¸‰ä¸ªç‹¬ç«‹å¢å¼ºæ¨¡å—**è¦†ç›–å…¨åœºæ™¯ï¼š

- **å®æ—¶å¯¹è¯**ï¼šæµå¼å·¥å…·è®©ç”¨æˆ·ä½“éªŒä»"ç­‰å¾…10ç§’"â†’"å®æ—¶çœ‹è¿›åº¦"
- **æ‰¹å¤„ç†**ï¼šå¼•æ“æ± è®©100ä»»åŠ¡ä»"ä¸²è¡Œ850ç§’"â†’"å¹¶è¡Œ120ç§’"
- **ä½å»¶è¿Ÿ**ï¼šç¼“å­˜è®©é¦–Tokenä»500msâ†’50ms

**æ ¸å¿ƒå“²å­¦**ï¼šä¸å› å¢å¼ºè€Œå¤æ‚åŒ–åŸºç¡€ï¼Œæ‰€æœ‰ä¼˜åŒ–é€šè¿‡**ä¾èµ–æ³¨å…¥**å®ç°ï¼Œä»£ç å¯éšæ—¶å›æ»šåˆ°180è¡Œçº¯å‡€ç‰ˆæœ¬ã€‚è¿™å°±æ˜¯**ç”Ÿäº§çº§æç®€**çš„çœŸè°›ã€‚